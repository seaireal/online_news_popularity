---
title: "Online News Popularity"
author: "Hui Wang, Shanshan Hu, and Zhichao Hu"
date: "4/1/2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width = 25,fig.height = 20)
library(tidyverse)
library(glmnet) # ridge and lasso
library(pls) # pcr pls
library(boot)
library(leaps) # regsubset
library(MASS)
library(class) # knn
library(reshape2) # melt the correlation matrix 
library(car) # vif, residual plots, partial residual plots
```

# Load data

```{r}
df = read_csv("OnlineNewsPopularity.csv")
```
# Prepare data

```{r}
# delete 2 non-predictive variables
# divide the shares into 2 groups: 0 - not popular, 1 - popular
df <- df %>% dplyr::select(-url, -timedelta) %>% 
  mutate(

    channel = case_when(
      data_channel_is_lifestyle == 1 ~ 1,
      data_channel_is_entertainment == 1 ~ 2,
      data_channel_is_bus == 1 ~ 3,
      data_channel_is_socmed == 1 ~ 4,
      data_channel_is_tech == 1 ~ 5,
      data_channel_is_world == 1 ~ 6,
      TRUE ~ 7
    ) %>% as_factor(),
    
    weekday = case_when(
      weekday_is_monday == 1 ~ 1,
      weekday_is_tuesday ==1 ~ 2,
      weekday_is_wednesday == 1 ~ 3,
      weekday_is_thursday == 1 ~ 4,
      weekday_is_friday == 1 ~ 5,
      weekday_is_saturday == 1 ~ 6,
      weekday_is_sunday == 1 ~ 7
    ) %>% as_factor(),
    
    ranks = case_when(
      shares > median(df$shares) ~ 1,
      shares <= median(df$shares) ~ 0
    ) %>% as_factor()
    
  ) %>% dplyr::select(-starts_with("data_channel_is"),
                      -starts_with("weekday_is"),
                      -channel, -weekday)

# remove channel as not all categories are covered (6134 unclassified)
# remove weekday as is_weekend variable captures this information

#table(df$ranks)
#colnames(df)[ apply(df, 2, anyNA) ] #check NA columns
```

# Create training and testing sets

```{r}
# split the data set into a training set and a test set
set.seed(123)
split_size = nrow(df)/5
testing = sample(1:nrow(df), split_size)
df_train = df[-testing, ]
df_test = df[testing, ]
```

# Create model matrix for Ridge, Lasso, PCR, and PLS
```{r}
# create model matrix
X_train = model.matrix(shares~.-ranks, df_train)[ ,-1] # remove intercept
Y_train = df_train$shares
X_test = model.matrix(shares~.-ranks, df_test)[ ,-1] # remove intercept
Y_test = df_test$shares
```

# Linear regression

```{r}
# linear regression

# heavily positive skewness
#stem(df$shares)
hist(log(df$shares))
hist(df$shares)

# correlation matrix heatmap
cormat <- cor(X_train, method = c("pearson"))
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()+
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 5, hjust = 1))+
 coord_fixed()
# Get lower triangle of the correlation matrix
  get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
  # Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
upper_tri <- get_upper_tri(cormat)
upper_tri
lower_tri <- get_lower_tri(cormat)
lower_tri
melted_upmat <- melt(upper_tri, na.rm = TRUE)
melted_lwmat <- melt(lower_tri, na.rm = TRUE)
ggplot(data = melted_upmat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 6, hjust = 1))+
 coord_fixed()
ggplot(data = melted_lwmat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 6, hjust = 1))+
 coord_fixed()

lmod <- lm(log(shares) ~ . -ranks, data=df)
summary(lmod)
lmod1 <- lm(shares ~ . -ranks, data=df)
summary(lmod1)
#coef(lmod) %>% length() # of predictors including intercept
vif(lmod)
vif(lmod1)
# from the description of the predictors and the sammry of the lm, we can easily know that 
# the model doesn't fit well at least because of the multicollinearity
```

```{r echo = TRUE,fig.width = 25,fig.height = 20}
# subset selection
lmod_formula = formula(lmod)
lmod_subset = regsubsets(lmod_formula, data=df, method="forward", nvmax=45)
lmod_subset_summary = summary(lmod_subset)

which.min(lmod_subset_summary$bic)
which.max(lmod_subset_summary$adjr2)

par(mfrow = c(1, 2))
plot(lmod_subset_summary$bic, xlab = "Subset Size", ylab = "BIC", pch = 20, type = "l",
     main = "BIC")
points(23,lmod_subset_summary$bic[23],col="red",cex=2,pch=20)
plot(lmod_subset_summary$adjr2, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20, type = "l",
     main = "Adjusted R2")
points(38,lmod_subset_summary$adjr2[38],col="red",cex=2,pch=20)

coef(lmod_subset, 23)

# the adjusted R2 is still too low, only around 2%, why?
# need to try other methods, such as ridge and lasso
lmod_formula = formula(lmod)
lmod_subset = regsubsets(lmod_formula, data=df, method="backward", nvmax=45)
lmod_subset_summary = summary(lmod_subset)

which.min(lmod_subset_summary$bic)
which.max(lmod_subset_summary$adjr2)
coef(lmod_subset, 27)
```

```{r}
lmod_bic = lm(log(shares)~
                num_hrefs+
                num_self_hrefs+
                num_imgs+
                average_token_length+
                num_keywords+
                kw_min_min+
                kw_max_min+
                kw_avg_min+
                kw_min_max+
                kw_avg_max+
                kw_min_avg+
                kw_max_avg+
                kw_avg_avg+
                self_reference_avg_sharess+
                is_weekend+
                LDA_00+
                LDA_01+
                LDA_04+
                global_subjectivity+
                min_positive_polarity+
                title_sentiment_polarity+
                abs_title_subjectivity+
                abs_title_sentiment_polarity,
              data=df_train
              )

summary(lmod_bic)

lmod_pred = predict(lmod_bic, df_test)
lmod_rmse = mean((lmod_pred-log(df_test$shares))^2) %>% sqrt()
lmod_rmse
vif(lmod_bic)
```

# PCR

```{r}
set.seed (1)
pcr.fit=pcr(log(shares)~.-ranks, data=df_train, scale=TRUE, validation ="CV")
summary (pcr.fit)
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit, X_test, ncomp =23)
sqrt(mean((as.numeric(pcr.pred)-Y_test)^2))
pcr.pred=predict(pcr.fit, X_test, ncomp =36)
mean((as.numeric(pcr.pred)-Y_test)^2)%>% sqrt()
pcr.pred=predict(pcr.fit, X_test, ncomp =43)
mean((as.numeric(pcr.pred)-Y_test)^2)%>% sqrt()
pcr.pred=predict(pcr.fit, X_test, ncomp =1) # smallest error
mean((as.numeric(pcr.pred)-Y_test)^2)%>% sqrt()
#pcr.m=pcr(shares~.-ranks, data=df,scale =TRUE ,ncomp =38)
#summary(pcr.m)
```

# PLS

```{r}
set.seed (1)
pls.fit=plsr(log(shares)~.-ranks, data=df_train, scale=TRUE, validation ="CV")
summary(pls.fit)
pls.pred=predict(pls.fit, X_test, ncomp =1)
mean((pls.pred -Y_test)^2)%>% sqrt()
pls.pred=predict(pls.fit, X_test, ncomp =7) # smallest error
mean((pls.pred -Y_test)^2)%>% sqrt()
pls.pred=predict(pls.fit, X_test, ncomp =11)
mean((pls.pred -Y_test)^2)%>% sqrt()
#pls.m=plsr(shares~.-ranks, data=df,scale=TRUE ,ncomp =4)
#summary(pls.m)
#pls.m=plsr(shares~.-ranks, data=df,scale=TRUE ,ncomp =43)
#summary(pls.m)
```

# Ridge

```{r}
set.seed(1)
ridge_cv = cv.glmnet(X_train, Y_train, alpha = 0)
ridge_lam = ridge_cv$lambda.min
ridge_mod = glmnet(X_train, Y_train, alpha = 0, lambda = ridge_lam)
ridge_pred = predict(ridge_mod, s=ridge_lam, newx=X_test)
ridge_rmse = mean((ridge_pred-Y_test)^2) %>% sqrt()
```

# Lasso

```{r}
set.seed(1)
lasso_cv = cv.glmnet(X_train, Y_train, alpha = 1)
lasso_lam = lasso_cv$lambda.min
lasso_mod = glmnet(X_train, Y_train, alpha = 1, lambda = lasso_lam)
lasso_pred = predict(lasso_mod, s=lasso_lam, newx=X_test)
lasso_rmse = mean((lasso_pred-Y_test)^2) %>% sqrt()
```

```{r}
coef(lasso_mod)
```
```{r}
X_train1 = model.matrix(shares~.-ranks-n_unique_tokens-n_non_stop_words-kw_min_min-kw_max_min-kw_min_max-kw_max_max-kw_min_avg-kw_max_avg-self_reference_min_shares-self_reference_max_shares-min_positive_polarity-max_positive_polarity-min_negative_polarity-max_negative_polarity, df_train)[ ,-1]
X_test1 = model.matrix(shares~.-ranks-n_unique_tokens-n_non_stop_words-kw_min_min-kw_max_min-kw_min_max-kw_max_max-kw_min_avg-kw_max_avg-self_reference_min_shares-self_reference_max_shares-min_positive_polarity-max_positive_polarity-min_negative_polarity-max_negative_polarity, df_test)[ ,-1]

set.seed(1)
lasso_cv = cv.glmnet(X_train1, Y_train, alpha = 1)
lasso_lam = lasso_cv$lambda.min
lasso_mod = glmnet(X_train1, Y_train, alpha = 1, lambda = lasso_lam)
lasso_pred = predict(lasso_mod, s=lasso_lam, newx=X_test1)
lasso_rmse = mean((lasso_pred-Y_test)^2) %>% sqrt()
```

```{r}
coef(lasso_mod)
```

# Logistic Regression
```{r}
modlg <- glm(ranks~
                n_tokens_title+
                n_tokens_content+
                num_hrefs+
                num_self_hrefs+
                num_imgs+
                average_token_length+
                num_keywords+
                kw_min_min+
                kw_avg_min+
                kw_min_max+
                kw_max_max+
                kw_avg_max+
                kw_min_avg+
                kw_max_avg+
                kw_avg_avg+
                self_reference_min_shares+
                self_reference_max_shares+
                is_weekend+
                LDA_01+
                LDA_02+
                LDA_03+
                global_subjectivity+
                global_rate_positive_words+
                min_negative_polarity+
                title_sentiment_polarity+
                abs_title_subjectivity+
                abs_title_sentiment_polarity, 
             data=df_train, family=binomial)
logp <- predict(modlg, df_test, type="response")
logpp <- ifelse(logp > 0.5, 1, 0)
table(logpp, df_test$ranks)
# test error
mean(logpp != df_test$ranks)
# sensitivity
sum(logpp==1& df_test$ranks==1)/sum(df_test$ranks==1)
# speficity
sum(logpp==0& df_test$ranks==0)/sum(df_test$ranks==0)
vif(modlg)
```

# LDA

```{r}
# perform LDA on the training data
# use predictors chosen by Lasso
lda_mod = lda(ranks ~ 
                n_tokens_title+
                n_tokens_content+
                num_hrefs+
                num_self_hrefs+
                num_imgs+
                average_token_length+
                num_keywords+
                kw_min_min+
                kw_avg_min+
                kw_min_max+
                kw_max_max+
                kw_avg_max+
                kw_min_avg+
                kw_max_avg+
                kw_avg_avg+
                self_reference_min_shares+
                self_reference_max_shares+
                is_weekend+
                LDA_01+
                LDA_02+
                LDA_03+
                global_subjectivity+
                global_rate_positive_words+
                min_negative_polarity+
                title_sentiment_polarity+
                abs_title_subjectivity+
                abs_title_sentiment_polarity, data = df_train)

#coef(lda_mod) %>% dim()
lda_pred = predict(lda_mod, df_test)
lda_ranks = lda_pred$class
test_ranks = df_test$ranks
print("confusion table")
table(lda_ranks, test_ranks)
print("test error rate")
mean(lda_ranks!= test_ranks)
# sensitivity
sum(lda_ranks==1& test_ranks==1)/sum(test_ranks==1)
# speficity
sum(lda_ranks==0& test_ranks==0)/sum(test_ranks==0)
```

# QDA

```{r}
# perform LDA on the training data
qda_mod = qda(ranks ~ 
                n_tokens_title+
                n_tokens_content+
                num_hrefs+
                num_self_hrefs+
                num_imgs+
                average_token_length+
                num_keywords+
                kw_min_min+
                kw_avg_min+
                kw_min_max+
                kw_max_max+
                kw_avg_max+
                kw_min_avg+
                kw_max_avg+
                kw_avg_avg+
                self_reference_min_shares+
                self_reference_max_shares+
                is_weekend+
                LDA_01+
                LDA_02+
                LDA_03+
                global_subjectivity+
                global_rate_positive_words+
                min_negative_polarity+
                title_sentiment_polarity+
                abs_title_subjectivity+
                abs_title_sentiment_polarity, data = df_train)
qda_pred = predict(qda_mod, df_test)
qda_ranks = qda_pred$class
test_ranks = df_test$ranks
print("confusion table")
table(qda_ranks, test_ranks)
print("test error rate")
mean(qda_ranks!=test_ranks)
# sensitivity
sum(qda_ranks==1& test_ranks==1)/sum(test_ranks==1)
# speficity
sum(qda_ranks==0& test_ranks==0)/sum(test_ranks==0)
```
# KNN

```{r}
X_train_knn = scale(X_train)
X_test_knn = scale(X_test)

# Using loop to test for K takes a long time, 9 is roughly the best
set.seed(1)
knn_ranks = knn(X_train_knn, X_test_knn, df_train$ranks, 9)
print("confusion table")
table(knn_ranks, test_ranks)
print("test error rate")
mean(knn_ranks!=test_ranks)
```

# Classification tree

```{r}
set.seed(1)
ptm = proc.time()
trmod = tree(ranks ~ .-shares, data=df_train)
time_spent = proc.time() - ptm
paste("Processing time:", round(time_spent[3]))
summary(trmod)
```

```{r}
# use lasso coefficients
set.seed(1)
lgmod1 <- glm(ranks~
                num_hrefs+
                num_imgs+
                num_keywords+
                kw_avg_avg+
                self_reference_avg_sharess+
                is_weekend+
                LDA_01+
                LDA_02,
             data=df_train, family="binomial")
formula_lg = formula(lgmod1)
lgmod1_pred <- predict(lgmod1, df_test, type="response")
lgmod1_ranks <- ifelse(lgmod1_pred > 0.5, 1, 0)
test_ranks = df_test$ranks
print("confusion table")
table(lgmod1_ranks, test_ranks)
print("test error")
mean(lgmod1_ranks != test_ranks)
print("sensitivity")
sum(lgmod1_ranks==1 & test_ranks==1)/sum(test_ranks==1)
print("specificity")
sum(lgmod1_ranks==0 & test_ranks==0)/sum(test_ranks==0)
```


```{r}
# use forward selection predictors
lgmod <- glm(ranks~
                num_hrefs+
                num_imgs+
                average_token_length+
                num_keywords+
                kw_avg_min+
                kw_avg_max+
                kw_avg_avg+
                self_reference_avg_sharess+
                is_weekend+
                LDA_01+
                LDA_02+
                LDA_03+
                global_subjectivity+
                title_sentiment_polarity,
             data=df_train, family="binomial")
formula_lg = formula(lgmod)
lgmod_pred <- predict(lgmod, df_test, type="response")
lgmod_ranks <- ifelse(lgmod_pred > 0.5, 1, 0)
test_ranks = df_test$ranks
print("confusion table")
table(lgmod_ranks, test_ranks)
print("test error")
mean(lgmod_ranks != test_ranks)
print("sensitivity")
sum(lgmod_ranks==1 & test_ranks==1)/sum(test_ranks==1)
print("specificity")
sum(lgmod_ranks==0 & test_ranks==0)/sum(test_ranks==0)

library(lmtest)
lmtest::lrtest(lgmod,lgmod1)
```

X_train_knn = df_train %>% dplyr::select(-shares, -ranks,-n_tokens_title,-n_tokens_content,-n_unique_tokens,-num_videos,-average_token_length,-kw_avg_min,-kw_avg_max,-LDA_00,-LDA_03,-LDA_04,-global_subjectivity,-global_sentiment_polarity,-global_rate_positive_words,-global_rate_negative_words,-avg_positive_polarity,-avg_negative_polarity,-title_subjectivity,-title_sentiment_polarity) %>% scale()
X_test_knn = df_test %>% dplyr::select(-shares, -ranks,-n_tokens_title,-n_tokens_content,-n_unique_tokens,-num_videos,-average_token_length,-kw_avg_min,-kw_avg_max,-LDA_00,-LDA_03,-LDA_04,-global_subjectivity,-global_sentiment_polarity,-global_rate_positive_words,-global_rate_negative_words,-avg_positive_polarity,-avg_negative_polarity,-title_subjectivity,-title_sentiment_polarity) %>% scale()
Y_train_knn = df_train$ranks


X_train_knn = df_train %>% dplyr::select(-shares, -ranks,-n_tokens_title,-n_tokens_content,-n_unique_tokens,-num_videos,-LDA_00,-LDA_04,-global_sentiment_polarity,-global_rate_positive_words,-global_rate_negative_words,-avg_positive_polarity,-avg_negative_polarity,-title_subjectivity) %>% scale()
X_test_knn = df_test %>% dplyr::select(-shares, -ranks,-n_tokens_title,-n_tokens_content,-n_unique_tokens,-num_videos,-LDA_00,-LDA_04,,-global_sentiment_polarity,-global_rate_positive_words,-global_rate_negative_words,-avg_positive_polarity,-avg_negative_polarity,-title_subjectivity) %>% scale()
Y_train_knn = df_train$ranks