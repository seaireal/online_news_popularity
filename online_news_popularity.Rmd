---
title: "Online News Popularity"
author: "Hui Wang, Shanshan Hu, and Zhichao Hu"
date: "4/1/2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet) # ridge and lasso
library(pls) 
library(boot)
library(leaps) # regsubset
library(MASS)
```

# Load data

```{r}
df = read_csv("OnlineNewsPopularity.csv")
```
# Prepare data

```{r}
# delete 2 non-predictive variables
# divide the shares into 3 groups: 1-high, 2-medium, and 3-low
df <- df %>% select(-url, -timedelta) %>% 
  mutate(

    channel = case_when(
      data_channel_is_lifestyle == 1 ~ 1,
      data_channel_is_entertainment == 1 ~ 2,
      data_channel_is_bus == 1 ~ 3,
      data_channel_is_socmed == 1 ~ 4,
      data_channel_is_tech == 1 ~ 5,
      data_channel_is_world == 1 ~ 6
    ) %>% as_factor(),
    
    weekday = case_when(
      weekday_is_monday == 1 ~ 1,
      weekday_is_tuesday ==1 ~ 2,
      weekday_is_wednesday == 1 ~ 3,
      weekday_is_thursday == 1 ~ 4,
      weekday_is_friday == 1 ~ 5,
      weekday_is_saturday == 1 ~ 6,
      weekday_is_sunday == 1 ~ 7
    ) %>% as_factor(),
    
    ranks = case_when(
      shares > quantile(df$shares,  probs = c(0.33,0.67))[[2]] ~ 1,
      shares < quantile(df$shares,  probs = c(0.33,0.67))[[1]] ~ 3,
      TRUE ~ 2
    ) %>% as_factor()
    
  ) %>% select(-starts_with("data_channel_is"), -starts_with("weekday_is"))

# table(df$ranks)
```

```{r}
# split the data set into a training set and a test set
set.seed(123)
split_size = nrow(df)/2
training = sample(1:nrow(df), split_size)
df_train = df[training, ]
df_test = df[-training, ]
```

# Linear regression

```{r}
# linear regression

plot(df$shares)   # heavily positive skewness

lmod <- lm(shares ~ ., data=df)
summary(lmod)

# from the description of the predictors and the sammry of the lm, we can easily know that 
# the model doesn't fit well at least because of the multicollinearity
```

```{r}
# subset selection
lmod_subset = regsubsets(shares ~ ., data=df, really.big=T)
lmod_subset_summary = summary(lmod_subset)

which.min(lmod_subset_summary$bic)
which.max(lmod_subset_summary$adjr2)

par(mfrow = c(1, 2))
plot(lmod_subset_summary$bic, xlab = "Subset Size", ylab = "BIC", pch = 20, type = "l",
main = "BIC of the Model")
plot(lmod_subset_summary$adjr2, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20,
type = "l", main = "adj_R2 of the Model")

coef(lmod_subset,9)

# the adjusted R2 is still too low, only around 2%, why?
# need to try other methods, such as ridge and lasso
```

# LDA

```{r}
# perform LDA on the training data
lda_mod = lda(ranks ~ . -shares, data = df_train)
#coef(lda_mod) %>% dim()
lda_pred = predict(lda_mod, df_test)
lda_ranks = lda_pred$class
test_ranks = df_test$ranks
print("confusion table")
table(lda_ranks, test_ranks)
print("test error rate")
mean(lda_ranks!=test_ranks)
```

# QDA

```{r}
# perform LDA on the training data
qda_mod = qda(ranks ~ . -shares, data = df_train)
qda_pred = predict(qda_mod, df_test)
qda_ranks = qda_pred$class
#test_ranks = df_test$ranks
print("confusion table")
table(qda_ranks, test_ranks)
print("test error rate")
mean(qda_ranks!=test_ranks)
```
